{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998ee6e6-2f76-456b-971b-51f5957d2168",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% md\n",
    "\n",
    "# Sprint20 ResNetとVGG\n",
    "\n",
    "このサンプルコードのフォルダ・ファイル構成の前提は下記になります。\n",
    "\n",
    "unet/\n",
    "\n",
    "    ・・・・\n",
    "    Sprint19.ipynb\n",
    "    Sprint20.ipynb\n",
    "    competition_data/\n",
    "        train/\n",
    "        test/\n",
    "        depths.csv\n",
    "    ・・・・\n",
    "    \n",
    "下記参考までに、詳細は各自調べてみてください。\n",
    "    \n",
    "resnet概要：https://www.bigdata-navi.com/aidrops/2611/\n",
    "\n",
    "vgg概要：https://aizine.ai/glossary-vgg/\n",
    "\n",
    "#%% md\n",
    "\n",
    "# ライブラリのimport\n",
    "\n",
    "#%%\n",
    "\n",
    "import gc\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.callbacks import *\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import *\n",
    "from keras.models import Model, load_model, save_model\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.applications.vgg19 import VGG19, preprocess_input\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 9)\n",
    "\n",
    "#%% md\n",
    "\n",
    "# データの読み込みと深さ情報の付与\n",
    "\n",
    "depths.csvに深さの情報が格納されているため、train.csvとsample_submission.csvに結合する。\n",
    "\n",
    "＊rle_maskについての説明：https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/61955\n",
    "\n",
    "#%%\n",
    "\n",
    "# 各種読み込み\n",
    "train_df = pd.read_csv('competition_data/train.csv')\n",
    "test_df = pd.read_csv('competition_data/sample_submission.csv')\n",
    "depth_df = pd.read_csv('competition_data/depths.csv')\n",
    "\n",
    "# 出力\n",
    "display(train_df.head())\n",
    "display(test_df.head())\n",
    "\n",
    "# depths.csvと結合\n",
    "train_df = train_df.merge(depth_df, how='left', on='id')\n",
    "test_df = test_df.merge(depth_df, how='left', on='id')\n",
    "\n",
    "# 出力\n",
    "display(train_df.head())\n",
    "display(test_df.head())\n",
    "\n",
    "#%% md\n",
    "\n",
    "# 画像データの読み込みとnumpy配列化\n",
    "\n",
    "#%%\n",
    "\n",
    "X_train = np.asarray(\n",
    "    [cv2.imread('competition_data/train/images/{}.png'.format(x), 0) for x in train_df.id.tolist()], \n",
    "    dtype=np.uint8) / 255.\n",
    "y_train = np.asarray(\n",
    "    [cv2.imread('competition_data/train/masks/{}.png'.format(x), 0) for x in train_df.id.tolist()],\n",
    "    dtype=np.uint8) / 255.\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "#%%\n",
    "\n",
    "# 試しに描画\n",
    "random_index = np.random.randint(0, X_train.shape[0])\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(X_train[random_index], cmap='gray')\n",
    "ax[1].imshow(y_train[random_index], cmap='gray')\n",
    "\n",
    "#%% md\n",
    "\n",
    "# セグメンテーションされている領域割合計算\n",
    "\n",
    "#%%\n",
    "\n",
    "def compute_coverage(df, masks):\n",
    "    \"\"\"領域割合の計算とクラスの付与\n",
    "    Parameters\n",
    "    ------------\n",
    "    train_df : df\n",
    "    masks : セグメンテーション画像\n",
    "    \"\"\"\n",
    "    def coverage_to_class(val):\n",
    "        \"\"\"領域割合からクラスを付与\n",
    "        val : セグメンテーション領域の割合\n",
    "        \"\"\"\n",
    "        for i in range(0, 11):\n",
    "            if val * 10 <= i:\n",
    "                return i\n",
    "\n",
    "    # セグメンテーション領域の割合\n",
    "    df['coverage'] = np.mean(masks, axis=(1, 2))\n",
    "    # 何割が覆われているかで、0~10までのクラスを与える\n",
    "    df['coverage_class'] = df.coverage.map(coverage_to_class)\n",
    "\n",
    "    return df\n",
    "\n",
    "# 関数実行\n",
    "compute_coverage(train_df, y_train)\n",
    "\n",
    "# 出力\n",
    "display(train_df)\n",
    "\n",
    "#%% md\n",
    "\n",
    "# データセットの準備\n",
    "\n",
    "#%%\n",
    "\n",
    "def create_depth_abs_channels(image_tensor):\n",
    "    \"\"\"3チャンネルに拡張\n",
    "    \"\"\"\n",
    "    image_tensor = image_tensor.astype(np.float32)\n",
    "    h, w, c = image_tensor.shape\n",
    "    for row, const in enumerate(np.linspace(0, 1, h)):\n",
    "        image_tensor[row, :, 1] = const\n",
    "    image_tensor[:, :, 2] = (\n",
    "        image_tensor[:, :, 0] * image_tensor[:, :, 1])\n",
    "\n",
    "    x_dx = np.diff(image_tensor[:, :, 0], axis=0)\n",
    "    x_dy = np.diff(image_tensor[:, :, 0], axis=1)\n",
    "    x_dx = cv2.copyMakeBorder(x_dx, 1, 0, 0, 0, cv2.BORDER_CONSTANT, 0)\n",
    "    x_dy = cv2.copyMakeBorder(x_dy, 0, 0, 1, 0, cv2.BORDER_CONSTANT, 0)\n",
    "    image_tensor[:, :, 1] = np.abs(x_dx + x_dy)\n",
    "    return image_tensor\n",
    "\n",
    "# 3チャンネルに拡張\n",
    "X_train_ch = np.repeat(np.expand_dims(X_train, axis=-1), 3, -1)\n",
    "X_train_ch = np.asarray(list(map(lambda x: create_depth_abs_channels(x), X_train_ch)))\n",
    "\n",
    "# ResNetのデフォルトのサイズに合わせるため、224,224にリサイズ\n",
    "X_resized = np.asarray(list(map(lambda x: cv2.resize(x, (224, 224)), X_train_ch)))\n",
    "y_resized = np.asarray(list(map(lambda x: cv2.resize(x, (224, 224)), y_train)))\n",
    "\n",
    "# train_test_splitでもよさそうだが、、、\n",
    "kfold = StratifiedKFold(n_splits=5)\n",
    "for train_index, valid_index in kfold.split(train_df.id.values, train_df.coverage_class.values):\n",
    "    X_tr, X_val = X_resized[train_index], X_resized[valid_index]\n",
    "    y_tr, y_val = y_resized[train_index], y_resized[valid_index]\n",
    "    break\n",
    "    \n",
    "# セグメンテーションの正解データの形整える\n",
    "y_tr = np.expand_dims(y_tr, axis=-1)\n",
    "y_val = np.expand_dims(y_val, axis=-1)\n",
    "\n",
    "print(X_resized.shape, y_resized.shape)\n",
    "print(X_tr.shape, y_tr.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "\n",
    "#%% md\n",
    "\n",
    "# 損失関数など定義\n",
    "\n",
    "#%%\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    \"\"\"dice損失の計算\n",
    "    DICE損失参考：https://qiita.com/ppza53893/items/8090322792e1c7f81e57\n",
    "    \"\"\"\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return 1. - score\n",
    "\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    \"\"\"binary_crossentropyとdice加算した統合損失\n",
    "    \"\"\"\n",
    "    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "\n",
    "def get_iou_vector(A, B):\n",
    "    \"\"\"IOUの計算\n",
    "    IOU参考：https://www.sigfoss.com/developer_blog/detail?actual_object_id=147\n",
    "    A : 正解データ\n",
    "    B : 0.5より大きければ1 0.5以下なら0に変換された予測領域\n",
    "    \"\"\"\n",
    "    # バッチ数取得\n",
    "    batch_size = A.shape[0]\n",
    "    # 最終的な評価指標\n",
    "    metric = 0.0\n",
    "    # バッチ数でループ\n",
    "    for batch in range(batch_size):\n",
    "        # 引数A,Bより配列抽出\n",
    "        t, p = A[batch], B[batch]\n",
    "        # 正しい領域のピクセル\n",
    "        true = np.sum(t) \n",
    "        # 推定した領域のピクセル\n",
    "        pred = np.sum(p) \n",
    "        \n",
    "        # deal with empty mask first\n",
    "        if true == 0:\n",
    "            metric += (pred == 0)\n",
    "            continue\n",
    "            \n",
    "        # 正しく領域と推定されたピクセル\n",
    "        intersection = np.sum(t * p) \n",
    "        \n",
    "        # IOU計算\n",
    "        union = true + pred - intersection\n",
    "        iou = intersection / union\n",
    "        \n",
    "        # 実際のIOUに近似\n",
    "        iou = np.floor(max(0, (iou - 0.45)*20)) / 10\n",
    "        \n",
    "        # 最終的な評価指標に加算\n",
    "        metric += iou\n",
    "        \n",
    "    # teake the average over all images in batch\n",
    "    metric /= batch_size\n",
    "    return metric\n",
    "\n",
    "\n",
    "def my_iou_metric(label, pred):\n",
    "    return tf.py_func(get_iou_vector, [label, pred>0.5], tf.float64)\n",
    "\n",
    "#%% md\n",
    "\n",
    "# resnet50モデルの定義\n",
    "\n",
    "#%%\n",
    "\n",
    "# 入力のshape定義\n",
    "input_size = (224, 224, 3)\n",
    "\n",
    "#%%\n",
    "\n",
    "# ライブラリのimportの際に読み込んでおいたResNet50をインスタンス化\n",
    "# 全結合層を場外する指定：include_top=False\n",
    "base_model = ResNet50(input_shape=input_size, include_top=False)\n",
    "base_model.summary()\n",
    "\n",
    "#%% md\n",
    "\n",
    "# デコーダーブロックの定義\n",
    "\n",
    "エンコーダーとデコーダーに関して、下記参照\n",
    "\n",
    "https://atmarkit.itmedia.co.jp/ait/articles/2007/10/news024.html\n",
    "\n",
    "#%%\n",
    "\n",
    "def decoder_block_simple(layer_name, block_name,num_filters=32,conv_dim=(3, 3)):\n",
    "    \"\"\"デコーダーブロック\n",
    "    Parameters\n",
    "    ------------\n",
    "    layer_name : 前の層のインスタンス\n",
    "    block_name : この層のベースとなる名前\n",
    "    num_filters : フィルター数\n",
    "    conv_dim : フィルターサイズ\n",
    "    \"\"\"\n",
    "    # 畳み込み\n",
    "    x_dec = Conv2D(num_filters, conv_dim,padding='same',name='{}_conv'.format(block_name))(layer_name)\n",
    "    # バッチ正規化\n",
    "    x_dec = BatchNormalization(name='{}_bn'.format(block_name))(x_dec)\n",
    "    # 活性化関数（PReLU）\n",
    "    x_dec = PReLU(name='{}_activation'.format(block_name))(x_dec)\n",
    "    return x_dec\n",
    "\n",
    "\n",
    "def decoder_block_bottleneck(layer_name, block_name,num_filters=32,conv_dim=(3, 3),dropout_frac=0.2):\n",
    "    \"\"\"複雑なデコーダーブロック\n",
    "    Parameters\n",
    "    ------------\n",
    "    layer_name : 前の層のインスタンス\n",
    "    block_name : この層のベースとなる名前\n",
    "    num_filters : フィルター数\n",
    "    conv_dim : フィルターサイズ\n",
    "    dropout_frac : ドロップアウト率\n",
    "    \"\"\"\n",
    "    x_dec = Conv2D(num_filters, conv_dim,padding='same',name='{}_conv1'.format(block_name))(layer_name)\n",
    "    x_dec = BatchNormalization(name='{}_bn1'.format(block_name))(x_dec)\n",
    "    x_dec = PReLU(name='{}_activation1'.format(block_name))(x_dec)\n",
    "    x_dec = Dropout(dropout_frac)(x_dec)\n",
    "\n",
    "    x_dec2 = Conv2D(num_filters // 2, conv_dim,padding='same',name='{}_conv2'.format(block_name))(x_dec)\n",
    "    x_dec2 = BatchNormalization(name='{}_bn2'.format(block_name))(x_dec2)\n",
    "    x_dec2 = PReLU(name='{}_activation2'.format(block_name))(x_dec2)\n",
    "    x_dec2 = Dropout(dropout_frac)(x_dec2)\n",
    "\n",
    "    x_dec2 = Conv2D(num_filters, conv_dim,padding='same',name='{}_conv3'.format(block_name))(x_dec2)\n",
    "    x_dec2 = BatchNormalization(name='{}_bn3'.format(block_name))(x_dec2)\n",
    "    x_dec2 = PReLU(name='{}_activation3'.format(block_name))(x_dec2)\n",
    "    x_dec2 = Dropout(dropout_frac)(x_dec2)\n",
    "\n",
    "    x_dec2 = Add()([x_dec, x_dec2])\n",
    "    return x_dec2\n",
    "\n",
    "#%% md\n",
    "\n",
    "# 全体モデルの定義\n",
    "\n",
    "#%%\n",
    "\n",
    "def unet_resnet(input_size, decoder_block,weights='imagenet',loss_func='binary_crossentropy',metrics_list=[my_iou_metric],use_lovash=False):\n",
    "    \"\"\"resnetを模倣したunet構造のネットワーク\n",
    "    Parameters\n",
    "    ------------\n",
    "    input_size    : 入力サイズ\n",
    "    decoder_block : デコーダーブロック\n",
    "    weights       : 重みの初期値　        default:imagenet,\n",
    "    loss_func     : 損失関数　            default:binary_crossentropy\n",
    "    metrics_list  : 指標一覧　            default:[my_iou_metric]\n",
    "    use_lovash    : lavash使うか          default:False\n",
    "    \"\"\"\n",
    "    # resnet50のベースモデル\n",
    "    base_model = ResNet50(input_shape=input_size, include_top=False, weights=weights)\n",
    "    \n",
    "    # base_modelの中間層を抽出\n",
    "    encoder1 = base_model.get_layer('conv1').output # activation_1\n",
    "    encoder2 = base_model.get_layer('res2c_branch2c').output # activation_10\n",
    "    encoder3 = base_model.get_layer('res3d_branch2c').output # activation_22\n",
    "    encoder4 = base_model.get_layer('res4f_branch2c').output # activation_40\n",
    "    encoder5 = base_model.get_layer('res5c_branch2c').output # activation_40\n",
    "    \n",
    "    # 流れ\n",
    "    # ①encoder5の出力をデコーダーブロックに渡す\n",
    "    # ②encoder5と①を結合\n",
    "    center = decoder_block(encoder5, 'center', num_filters=512)\n",
    "    concat5 = concatenate([center, encoder5], axis=-1)\n",
    "\n",
    "    # 流れ\n",
    "    # ①concat5,の出力をデコーダーブロックに渡す\n",
    "    # ②①をupsamplingする\n",
    "    # ③encoder4と②を結合する\n",
    "    # ①～③を繰り返し\n",
    "    decoder4 = decoder_block(concat5, 'decoder4', num_filters=256)\n",
    "    concat4 = concatenate([UpSampling2D()(decoder4), encoder4], axis=-1)\n",
    "\n",
    "    decoder3 = decoder_block(concat4, 'decoder3', num_filters=128)\n",
    "    concat3 = concatenate([UpSampling2D()(decoder3), encoder3], axis=-1)\n",
    "\n",
    "    decoder2 = decoder_block(concat3, 'decoder2', num_filters=64)\n",
    "    concat2 = concatenate([UpSampling2D()(decoder2), encoder2], axis=-1)\n",
    "\n",
    "    decoder1 = decoder_block(concat2, 'decoder1', num_filters=64)\n",
    "    concat1 = concatenate([UpSampling2D()(decoder1), encoder1], axis=-1)\n",
    "\n",
    "    # 再度upsamplingする\n",
    "    output = UpSampling2D()(concat1)\n",
    "    # デコーダーブロックに通す\n",
    "    output = decoder_block(output, 'decoder_output', num_filters=32)\n",
    "    # 畳込み\n",
    "    output = Conv2D(1, (1, 1), activation=None, name='prediction')(output)\n",
    "    if not use_lovash:\n",
    "        # lovash使う場合は、sigmoid関数に通しておく\n",
    "        output = Activation('sigmoid')(output)\n",
    "        \n",
    "    \n",
    "    # モデルの結合\n",
    "    model = Model(base_model.input, output)\n",
    "    # 損失・最適化手法・指標一覧を与えて、コンパイル\n",
    "    model.compile(loss=loss_func, optimizer='adam', metrics=metrics_list)\n",
    "    \n",
    "    # モデルを返す\n",
    "    return model\n",
    "\n",
    "#%% md\n",
    "\n",
    "# 学習\n",
    "\n",
    "#%%\n",
    "\n",
    "# モデルの定義\n",
    "model = unet_resnet(input_size, decoder_block_bottleneck, weights='imagenet',loss_func=bce_dice_loss, metrics_list=[my_iou_metric],use_lovash=False)\n",
    "print(model.summary())\n",
    "\n",
    "#%%\n",
    "\n",
    "# 学習回数\n",
    "EPOCHS = 2\n",
    "# バッチ数\n",
    "BATCH_NUM = 16\n",
    "\n",
    "# モデルの保存設定\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    'unet_resnet.h5',\n",
    "    monitor='val_my_iou_metric',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "# 学習率の減衰設定\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_my_iou_metric',\n",
    "    mode='max',\n",
    "    factor=0.5, \n",
    "    patience=5, \n",
    "    min_lr=0.0001, \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# 学習の実行と学習過程の取得\n",
    "history = model.fit(\n",
    "    X_tr, y_tr,\n",
    "    validation_data=[X_val, y_val], \n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_NUM,\n",
    "    callbacks=[model_checkpoint,reduce_lr], \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "#%% md\n",
    "\n",
    "# 推定\n",
    "\n",
    "#%%\n",
    "\n",
    "# 推定\n",
    "val_preds = model.predict(X_val, batch_size=16)\n",
    "\n",
    "# 元の大きさにリサイズ\n",
    "y_val_pred = np.asarray(list(map(lambda x: cv2.resize(x, (101, 101)), val_preds)))\n",
    "y_val_true = np.asarray(list(map(lambda x: cv2.resize(x, (101, 101)), y_val)))\n",
    "\n",
    "#%%\n",
    "\n",
    "# 描画\n",
    "random_index = np.random.randint(0, y_val_pred.shape[0])\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(y_val_pred[random_index], cmap='gray')\n",
    "ax[1].imshow(y_val_true[random_index], cmap='gray')\n",
    "\n",
    "#%% md\n",
    "\n",
    "# 上記コードのVGG対応\n",
    "\n",
    "#%%\n",
    "\n",
    "def unet_vgg(input_size, decoder_block,weights='imagenet',loss_func='binary_crossentropy',metrics_list=[my_iou_metric],use_lovash=False):\n",
    "    \"\"\"説明省略\"\"\"\n",
    "    base_model = VGG19(input_shape=input_size, include_top=False,weights=weights)\n",
    "    \n",
    "    encoder1 = base_model.get_layer('block1_conv2').output # (224,224,64)\n",
    "    encoder2 = base_model.get_layer('block2_conv2').output # (112,112,228)\n",
    "    encoder3 = base_model.get_layer('block3_conv4').output # (56,56,256)\n",
    "    encoder4 = base_model.get_layer('block4_conv4').output # (28,28,512)\n",
    "    encoder5 = base_model.get_layer('block5_conv4').output # (14,14,512)\n",
    "\n",
    "\n",
    "    center = decoder_block(encoder5, 'center', num_filters=512)\n",
    "    concat5 = concatenate([center, encoder5], axis=-1) # (14,14,1024)\n",
    "\n",
    "    decoder4 = decoder_block(concat5, 'decoder4', num_filters=256) \n",
    "    concat4 = concatenate([UpSampling2D()(decoder4), encoder4], axis=-1) \n",
    "    \n",
    "    decoder3 = decoder_block(concat4, 'decoder3', num_filters=128) \n",
    "    concat3 = concatenate([UpSampling2D()(decoder3), encoder3], axis=-1) \n",
    "\n",
    "    decoder2 = decoder_block(concat3, 'decoder2', num_filters=64)\n",
    "    concat2 = concatenate([UpSampling2D()(decoder2), encoder2], axis=-1)\n",
    "\n",
    "    decoder1 = decoder_block(concat2, 'decoder1', num_filters=64)\n",
    "    concat1 = concatenate([UpSampling2D()(decoder1), encoder1], axis=-1)\n",
    "    \n",
    "    output = decoder_block(concat1, 'decoder_output', num_filters=32)\n",
    "    output = Conv2D(1, (1, 1), activation=None, name='prediction')(output)\n",
    "    if not use_lovash:\n",
    "        output = Activation('sigmoid')(output)\n",
    "        \n",
    "    model = Model(base_model.input, output)\n",
    "    model.compile(loss=loss_func, optimizer='adam', metrics=metrics_list)\n",
    "\n",
    "    return model\n",
    "\n",
    "#%%\n",
    "\n",
    "# モデルの定義\n",
    "model = unet_vgg(input_size, decoder_block_bottleneck, weights='imagenet',loss_func=bce_dice_loss, metrics_list=[my_iou_metric],use_lovash=False)\n",
    "print(model.summary())\n",
    "\n",
    "#%%\n",
    "\n",
    "# 学習回数\n",
    "EPOCHS = 2\n",
    "# バッチ数\n",
    "BATCH_NUM = 16\n",
    "\n",
    "# モデルの保存設定\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    'unet_resnet.h5',\n",
    "    monitor='val_my_iou_metric',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "# 学習率の減衰設定\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_my_iou_metric',\n",
    "    mode='max',\n",
    "    factor=0.5, \n",
    "    patience=5, \n",
    "    min_lr=0.0001, \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# 学習の実行と学習過程の取得\n",
    "history = model.fit(\n",
    "    X_tr, y_tr,\n",
    "    validation_data=[X_val, y_val], \n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_NUM,\n",
    "    callbacks=[model_checkpoint,reduce_lr], \n",
    "    verbose=1\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
