{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bd46d2b-9950-4036-96ce-4f6cf596648f",
   "metadata": {},
   "source": [
    "# 線形回帰とは\n",
    "\n",
    "## ゴール\n",
    "- **【Sprint 機械学習スクラッチ 線形回帰】問題１を解くうえで必要な知識や技術について理解する**\n",
    "\n",
    "## このヒント集について\n",
    "### Sprintの目的\n",
    "- 線形回帰の意味を理解する\n",
    "- 線形回帰をスクラッチ開発するのに必要な概念を理解する\n",
    "\n",
    "## どのように学ぶか\n",
    "\n",
    "【Sprint 機械学習スクラッチ 線形回帰】の目次と照らし合わせながら、進めていきましょう。\n",
    "\n",
    "### 線形回帰とは\n",
    "**線形回帰**とは、目的変数(y)と説明変数(x)を$y=ax+b$という関係式で近似的に当てはめることです。\n",
    "\n",
    "$y=ax+b$と書ける式はxy座標で直線を描きます。以下画像のようなイメージです。\n",
    "\n",
    "<a href=\"https://diveintocode.gyazo.com/f07c4804230471314cc50ba61b0dd627\"><img src=\"https://t.gyazo.com/teams/diveintocode/f07c4804230471314cc50ba61b0dd627.png\" alt=\"Image from Gyazo\" width=\"800\"/></a>\n",
    "\n",
    "\n",
    "### スクラッチコードの完成形\n",
    "線形回帰のクラスをスクラッチで作成していきますが、最終的なコードはどのようになっているのでしょうか。\n",
    "\n",
    "下記は、最終的なコードの概観になります。\n",
    "\n",
    "```python\n",
    "class ScratchLinearRegression():\n",
    "    def __init__(self,・・・):\n",
    "      \"\"\"\n",
    "      インスタンス変数初期化\n",
    "      \"\"\"\n",
    "      ・・・\n",
    "\n",
    "    # 問題6（学習と推定）\n",
    "    def fit(self,・・・):\n",
    "        \"\"\"\n",
    "        線形回帰の学習\n",
    "        \"\"\"\n",
    "        # メイン処理\n",
    "        for i in range(学習回数):\n",
    "            # 問題1（過程関数の計算）\n",
    "            self._linear_hypothesis(・・・)\n",
    "            \n",
    "            # 問題2（最急降下法によるパラメータの更新値計算）\n",
    "            self._gradient_descent(・・・)\n",
    "\n",
    "            # 問題7（学習曲線のプロット）のグラフ描画時（問題5（損失関数）で作成した関数を使用）\n",
    "            self._loss_func(・・・)\n",
    "\n",
    "\n",
    "    # 問題1\n",
    "    def _linear_hypothesis(self,・・・):\n",
    "        \"\"\"\n",
    "        仮定関数の計算\n",
    "        \"\"\"\n",
    "\n",
    "    # 問題2\n",
    "    def _gradient_descent(self,・・・):\n",
    "        \"\"\"\n",
    "        最急降下法によるパラメータの更新値計算\n",
    "        \"\"\"\n",
    "\n",
    "    # 問題3\n",
    "    def predict(self,・・・):\n",
    "        \"\"\"\n",
    "        線形回帰での推定\n",
    "        \"\"\"\n",
    "\n",
    "    # 問題4\n",
    "    def _mse(self,・・・):\n",
    "        \"\"\"\n",
    "        平均二乗誤差の計算\n",
    "        \"\"\"\n",
    "\n",
    "    # 問題5\n",
    "    def _loss_func(self,・・・):\n",
    "        \"\"\"\n",
    "        損失関数\n",
    "        \"\"\"\n",
    "        # 問題4\n",
    "        self._mse(・・・)\n",
    "        \n",
    "\n",
    "```\n",
    "\n",
    "### 線形回帰のイメージ\n",
    "\n",
    "HousePriceデータセットで考えてみます。\n",
    "\n",
    "目的変数`y(HousePrice)`に対し、ひとつ説明変数`x1(GrLivArea)`を選び、2変数間の関係をプロットしてみると下記のようなグラフになります。\n",
    "\n",
    "<a href=\"https://diveintocode.gyazo.com/b3e0593438c1986b900f7da3bc02a7e0\"><img src=\"https://t.gyazo.com/teams/diveintocode/b3e0593438c1986b900f7da3bc02a7e0.png\" alt=\"Image from Gyazo\" width=\"800\"/></a>\n",
    "\n",
    "\n",
    "プロットされたデータの傾向をみると、「なにやら直線が引けそう」ということがわかるかと思います。これを**線形関係にある**と言い、下記のような直線を引くことが出来ます。\n",
    "\n",
    "<a href=\"https://diveintocode.gyazo.com/dee97b529a9956d82d375c10f88056a8\"><img src=\"https://t.gyazo.com/teams/diveintocode/dee97b529a9956d82d375c10f88056a8.png\" alt=\"Image from Gyazo\" width=\"800\"/></a>\n",
    "\n",
    "\n",
    "上記のグラフの直線を一般化して数式に直すと、次のようになります。\n",
    "\n",
    "$$\n",
    "y = a x_1 + b\n",
    "$$\n",
    "\n",
    "データに対して、最も当てはまりの良い`a`と`b`を求めることを、**回帰式を解く**と言います。そして、この式は、説明変数の数が1つの場合の式なので、**線形単回帰**になります。\n",
    "\n",
    "## まとめ\n",
    "- 線形回帰とは目的変数(y)が説明変数(x)にどれほど依存しているかあらわすモデルです\n",
    "- 説明変数が1つのものを線形単回帰モデルと呼び、2つ以上あるものを線形重回帰モデルと呼びます\n",
    "- 線形単回帰モデルは一般に$y = a x_1 + b$と記述できます"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2faa174-aaa4-42d2-8c13-2795a94f9b96",
   "metadata": {},
   "source": [
    "# 仮定関数とは\n",
    "\n",
    "## ゴール\n",
    "- **【Sprint 機械学習スクラッチ 線形回帰】の【問題１】を解くうえで必要な知識や技術について理解する**\n",
    "\n",
    "### Sprintの目的\n",
    "- 仮定関数の意味を理解する\n",
    "- 線形回帰をスクラッチ開発するのに必要な転置行列や行列積について理解する\n",
    "- 線形回帰をスクラッチ実装する際のより踏み込んだヒントを掲載しているので、それを基に、仮定関数をスクラッチ実装できるようになる。\n",
    "\n",
    "## どのように学ぶか\n",
    "\n",
    "【Sprint 機械学習スクラッチ 線形回帰】の【問題１】と照らし合わせながら、進めていきましょう。\n",
    "\n",
    "## 仮定関数とは\n",
    "\n",
    "これまで学んできた`y = ax1 + b`を`仮定関数`と呼びます。\n",
    "\n",
    "上記の例では、変数が1つだけでしたが、複数の変数がある**線形重回帰**の場合、下記のような式になります。\n",
    "\n",
    "$$\n",
    "h_\\theta(x) =  \\theta_0 x_0 + \\theta_1 x_1 + ... + \\theta_j x_j + ... +\\theta_n x_n.   (x_0 = 1)\\\\\n",
    "$$å\n",
    "\n",
    "これを変形すると下記のようになります。\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\theta^T \\cdot x.\n",
    "$$\n",
    "\n",
    "この数式は、どのように導出されたのでしょうか。\n",
    "\n",
    "### 転置行列積\n",
    "\n",
    "転置行列積が`y = ax1 + b`の計算結果と一致することを確認してみましょう。\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "#y=ax1+b\n",
    "a = 1\n",
    "b = 2\n",
    "x1 = 3\n",
    "y = a*x1 + b\n",
    "print(y)\n",
    "\n",
    "#転置行列積\n",
    "theta = np.array([[2],[1]])\n",
    "X = np.array([[1,3]])\n",
    "y = theta.T @ X\n",
    "print(y)\n",
    "```\n",
    "bにあたる部分をthetaの第1成分とし,入力Xの第1成分を1とすることで等価な処理を行えます。\n",
    "行列計算に置き換えることで、計算時間が短縮されるので、行列計算に落とし込める場合はできるだけ、行列計算に落とし込みましょう。\n",
    "\n",
    "また、機械学習では2つのデータだけでなく複数のデータを一挙に取り扱う事が多いです。そういった場合にはXは2次元配列になります。\n",
    "以下のコードで実験してみましょう。\n",
    "\n",
    "```python\n",
    "X = np.array([[1,1],\n",
    "              [1,2],\n",
    "              [1,3]])\n",
    "y = theta.T @ X.T\n",
    "```\n",
    "\n",
    "\n",
    "つまり、\n",
    "$$\n",
    "h_\\theta(x) =  \\theta_0 x_0 + \\theta_1 x_1 + ... + \\theta_j x_j + ... +\\theta_n x_n.   (x_0 = 1)\\\\\n",
    "$$\n",
    "は\n",
    "$$\n",
    "h_\\theta(x) = \\theta^T \\cdot x.\n",
    "$$\n",
    "と記述できます（記述方法に関しては、数学の世界で決まっていることなので、転置行列積はこのように表記すると覚えてください）\n",
    "\n",
    "### トイデータ\n",
    "\n",
    "作成した関数に、下記の変数を引数として与えてみましょう。（係数は a=2,b=3とします）\n",
    "\n",
    "```python\n",
    "X = np.arange(10)\n",
    "```\n",
    "\n",
    "戻り値として、下記の出力があれば、正常に作成できています。\n",
    "\n",
    "```python\n",
    "y = [3,5,7,9,11,13,15,17,19,21]\n",
    "```\n",
    "\n",
    "## まとめ\n",
    "- 仮定関数はこちら側が仮定するモデル(近似関数)になります\n",
    "- ここでは近似関数は`y = ax1 + b`と書けるものでした\n",
    "- 近似関数は転置行列を用いて計算できます"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1588acd5-1e18-4aea-ab8e-d987b7f6ad03",
   "metadata": {},
   "source": [
    "# 最急降下法とは\n",
    "\n",
    "## ゴール\n",
    "- **【Sprint 機械学習スクラッチ 線形回帰】の【問題2】を解くうえで必要な知識や技術について理解する**\n",
    "\n",
    "### Sprintの目的\n",
    "- 線形回帰の意味を理解する\n",
    "- 線形回帰をスクラッチ開発するのに必要な概念を理解する\n",
    "\n",
    "## どのように学ぶか\n",
    "\n",
    "【Sprint 機械学習スクラッチ 線形回帰】の目次と照らし合わせながら、進めていきましょう。\n",
    "\n",
    "## 【問題2】最急降下法\n",
    "**回帰式を解く**とは、データにもっとも当てはまりの良い、Θi(i=複数変数ある際の変数の番号)の値を求めることだと紹介しましたが、どのように、このΘを求めればいいのでしょうか。\n",
    "\n",
    "### 誤差\n",
    "\n",
    "**データにもっとも当てはまりが良い**とは、下記の図で言う`error`の値が、最も小さくなる時のことを言います。\n",
    "\n",
    "<a href=\"https://diveintocode.gyazo.com/dc4108d0eea9b579cb67ebbd8a5a7d92\"><img src=\"https://t.gyazo.com/teams/diveintocode/dc4108d0eea9b579cb67ebbd8a5a7d92.png\" alt=\"Image from Gyazo\" width=\"800\"/></a>\n",
    "\n",
    "### 平均2乗誤差\n",
    "\n",
    "引く直線によっては、`error`の値が±両方出てきますので、単純なerrorの合計値だけで、**データにもっとも当てはまりが良い**状態を判断することが出来ません。\n",
    "\n",
    "そこで、使用されるのが、平均2乗誤差と呼ばれるもので、下記の式で表されます。\n",
    "\n",
    "$$\n",
    "L(\\theta)=  \\frac{1 }{ m}  \\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^2.\n",
    "$$\n",
    "\n",
    "この数式の値が最小になるようなΘを求めることが、回帰式の**ゴール**となります。\n",
    "\n",
    "### 最急降下法（Θの求め方）\n",
    "\n",
    "例えば、平均2乗誤差の数式をグラフにプロットした場合、下記のようになったとします。\n",
    "\n",
    "<a href=\"https://diveintocode.gyazo.com/607d80ca9eebee99b564c1e47c946896\"><img src=\"https://t.gyazo.com/teams/diveintocode/607d80ca9eebee99b564c1e47c946896.png\" alt=\"Image from Gyazo\" width=\"800\"/></a>\n",
    "\n",
    "平均2乗誤差の値が最小になる地点は、次の位置であるとわかるかと思います。\n",
    "\n",
    "<a href=\"https://diveintocode.gyazo.com/704481108d13936ee159071457fd114c\"><img src=\"https://t.gyazo.com/teams/diveintocode/704481108d13936ee159071457fd114c.png\" alt=\"Image from Gyazo\" width=\"800\"/></a>\n",
    "\n",
    "このΘの個所をどのように特定すればいいのでしょうか。下記の手順になります。\n",
    "\n",
    "①まずは、ランダムにΘをプロットします。\n",
    "\n",
    "<a href=\"https://diveintocode.gyazo.com/2dcbcbe726bcb342cc003fccbfebf8a3\"><img src=\"https://t.gyazo.com/teams/diveintocode/2dcbcbe726bcb342cc003fccbfebf8a3.png\" alt=\"Image from Gyazo\" width=\"800\"/></a>\n",
    "\n",
    "②ランダムにプロットされたシータの位置の傾きを求める\n",
    "\n",
    "<a href=\"https://diveintocode.gyazo.com/b691334882a9eb7de2ea145e62a8e28d\"><img src=\"https://t.gyazo.com/teams/diveintocode/6b691334882a9eb7de2ea145e62a8e28d.png\" alt=\"Image from Gyazo\" width=\"800\"/></a>\n",
    "\n",
    "③傾きと学習率により、Θを更新\n",
    "\n",
    "<a href=\"https://diveintocode.gyazo.com/60b2742d1713b8c3fef99b38ec62443b\"><img src=\"https://t.gyazo.com/teams/diveintocode/60b2742d1713b8c3fef99b38ec62443b.png\" alt=\"Image from Gyazo\" width=\"800\"/></a>\n",
    "\n",
    "④①～③を繰り返す\n",
    "\n",
    "<a href=\"https://diveintocode.gyazo.com/328d2159009b54896252805af22e0f84\"><img src=\"https://t.gyazo.com/teams/diveintocode/328d2159009b54896252805af22e0f84.png\" alt=\"Image from Gyazo\" width=\"800\"/></a>\n",
    "\n",
    "**これを数式で表すと下記のようになります。**\n",
    "\n",
    "$$\n",
    "J(\\theta)=  \\frac{1 }{ 2m}  \\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^2.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}[(h_\\theta(x^{(i)}) - y^{(i)} )x_{j}^{(i)}]\n",
    "$$\n",
    "\n",
    "\n",
    "- ヒント：パラメータ導出のイメージ（https://diver.diveintocode.jp/questions/8028）\n",
    "- ヒント：なぜ降下法を使うのか（https://diver.diveintocode.jp/questions/8029）\n",
    "\n",
    "\n",
    "### 損失関数（目的関数）\n",
    "\n",
    "平均2乗誤差を最小にするΘを求めてきましたが、この対象となる誤差関数のことを、**損失関数（目的関数）**といいます。\n",
    "\n",
    "Θの更新の際に、平均2乗誤差を使用していると紹介しましたが、厳密には、展開後の式を分かりやすくするため、下記の数式を利用しています。\n",
    "\n",
    "$$\n",
    "L(\\theta)=  \\frac{1 }{ m}  \\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^2.\n",
    "$$\n",
    "\n",
    "↓\n",
    "\n",
    "$$\n",
    "J(\\theta)=  \\frac{1 }{ 2m}  \\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^2.\n",
    "$$\n",
    "\n",
    "どちらもΘについて微分し、係数を学習率に飲み込ませることで等価な式となります。\n",
    "\n",
    "### トイデータ\n",
    "\n",
    "作成した関数に、下記の変数を引数として与えてみましょう。\n",
    "\n",
    "まずは誤差の含まないモデルで実験してみましょう。\n",
    "例えば適当に以下のデータを利用します。\n",
    "```python\n",
    "x = np.linspace(1,6,5)\n",
    "X = np.c_[np.ones(5),x]#入力データX\n",
    "\n",
    "y = 2*x + 1#適当な真のモデル\n",
    "\n",
    "theta = [0,0]#仮定関数の係数の初期値\n",
    "y_pred = X @ theta\n",
    "\n",
    "error = y_pred - y#入力データerror\n",
    "```\n",
    "\n",
    "更新式を実装し、学習率alpha=0.05の時、更新された後のthetaが以下の様になっていれば成功です。\n",
    "\n",
    "```\n",
    "theta:[0.8, 3.1450000000000005]\n",
    "```\n",
    "以下の更新式を、行列計算で書いた場合は\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}[(h_\\theta(x^{(i)}) - y^{(i)} )x_{j}^{(i)}]\n",
    "$$\n",
    "\n",
    "```\n",
    "theta:[0.8, 3.425]\n",
    "```\n",
    "となります。\n",
    "\n",
    "この違いはパラメータ更新を同時に行うか、1つずつ行うかという違いになります。\n",
    "\n",
    "## まとめ\n",
    "- 最急降下法とは`y = ax1 + b`における`a`と`b`を最適な値にする手法です\n",
    "- 平均2乗誤差(損失関数)を微分することで更新式が導出されることを学びました\n",
    "- 更新過程のイメージは平均2乗誤差の山を降っていく様なイメージです"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e849d075-05f9-4883-aed4-9c48de67d94b",
   "metadata": {},
   "source": [
    "# 推定とは\n",
    "## ゴール\n",
    "- **【Sprint 機械学習スクラッチ 線形回帰】を解くうえで必要な知識や技術について理解する**\n",
    "\n",
    "## このヒント集について\n",
    "### Sprintの目的\n",
    "- 線形回帰における推定の概念を理解する\n",
    "\n",
    "### どのように学ぶか\n",
    "\n",
    "【Sprint 機械学習スクラッチ 線形回帰】の目次と照らし合わせながら、進めていきましょう。\n",
    "\n",
    "## 推定\n",
    "### 推定とは\n",
    "パラメータを決定した後、そのパラメータを用いて、予測結果を出力しなければなりません。これを推定と言います。\n",
    "\n",
    "## 【問題3】推定\n",
    "\n",
    "【問題1】で既に、仮定関数の出力は実装しているかと思いますが、【問題1】で作成した関数は、クラスの外から呼び出せない仕様になっています。\n",
    "```predict```メソッドを作成して、予測値を出力できる様にしましょう。```predict```メソッドで更新されたパラメータを使うためには```self```を利用しましょう。\n",
    "\n",
    "## まとめ\n",
    "- 推定とは更新されたパラメータを利用して予測値を出力することです"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35ab5c4-502d-452f-81c8-231f4656e44e",
   "metadata": {},
   "source": [
    "# 平均二乗誤差　スクラッチ\n",
    "## ゴール\n",
    "- **【Sprint 機械学習スクラッチ 線形回帰】の【問題４】を解くうえで必要な知識や技術について理解する**\n",
    "\n",
    "### Sprintの目的\n",
    "- 平均二乗誤差について理解する\n",
    "- 平均二乗誤差の実装方法について理解する\n",
    "\n",
    "## どのように学ぶか\n",
    "\n",
    "【Sprint 機械学習スクラッチ 線形回帰】の【問題４】と照らし合わせながら、進めていきましょう。\n",
    "\n",
    "## 【問題4】平均二乗誤差\n",
    "\n",
    "### 平均二乗誤差とは\n",
    "平均二乗誤差とは教師データと予測値の残差を二乗し、訓練データ全ての点に対して足して平均を取ったものです。\n",
    "\n",
    "\n",
    "式を分解して考えてみると、計算の順番は下記になるかと思います。\n",
    "\n",
    "1. 推定結果を計算\n",
    "\n",
    "$$\n",
    "h_\\theta(x_i) = \\theta^T \\cdot x_i\n",
    "$$\n",
    "\n",
    "2. 実測値との差を計算【対応する数式の個所を書いてください】\n",
    "\n",
    "$$\n",
    "error_i = h_\\theta(x_i) - y_i\n",
    "$$\n",
    "\n",
    "3. 1,2の2乗を計算\n",
    "\n",
    "$$\n",
    "squared error_i = error_i^2\n",
    "$$\n",
    "\n",
    "4. 3の合計値を計算【対応する数式の個所を書いてください】\n",
    "\n",
    "$$\n",
    "sum squared error = \\sum_{i=1}^{m} squared error_i\n",
    "$$\n",
    "\n",
    "5. データの長さで割って4の平均値を計算\n",
    "\n",
    "$$\n",
    "mean squared error = \\sum_{i=1}^{m} squared error_i\n",
    "$$\n",
    "\n",
    "ひな形では、推定結果と教師データが既に与えられるようにしています。\n",
    "\n",
    "ヒント：numpyの各種関数を利用してみましょう。\n",
    "\n",
    "### トイデータ\n",
    "\n",
    "作成した関数に、下記の変数を引数として与えてみましょう。\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "y_pred = np.array([0,1,2,3,4,5])\n",
    "y = np.array([1,3,5,7,9,11])\n",
    "```\n",
    "\n",
    "戻り値として、下記の出力があれば、正常に作成できています。\n",
    "\n",
    "```python\n",
    "11.0\n",
    "```\n",
    "\n",
    "## まとめ\n",
    "- 平均二乗誤差は教師データと予測結果の残差を二乗し、訓練データ全てについって計算したものです\n",
    "-　numpyを使うことで簡単に計算することができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e238bde2-55bd-4c49-ace2-45b73078e699",
   "metadata": {},
   "source": [
    "# 目的関数　スクラッチ\n",
    "## ゴール\n",
    "- **【Sprint 機械学習スクラッチ 線形回帰】の【問題5】を解くうえで必要な知識や技術について理解する**\n",
    "\n",
    "### Sprintの目的\n",
    "- 目的関数について理解する\n",
    "- 目的関数の実装方法について理解する\n",
    "\n",
    "## どのように学ぶか\n",
    "\n",
    "【Sprint 機械学習スクラッチ 線形回帰】の【問題5】と照らし合わせながら、進めていきましょう。\n",
    "\n",
    "\n",
    "## 【問題5】目的関数\n",
    "\n",
    "目的関数は以下の式で表せます。\n",
    "$$\n",
    "J(\\theta)=  \\frac{1 }{ 2m}  \\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^2.\n",
    "$$\n",
    "平均二乗誤差をさらに2で割った式になります。最小化させるべき目的の関数という意味で目的関数と呼ばれます。\n",
    "\n",
    "### トイデータ\n",
    "\n",
    "作成した関数に、下記の変数を引数として与えてみましょう。\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "y_pred = np.array([0,1,2,3,4,5])\n",
    "y = np.array([1,3,5,7,9,11])\n",
    "```\n",
    "\n",
    "戻り値として、下記の出力があれば、正常に作成できています。\n",
    "\n",
    "```python\n",
    "5.5\n",
    "```\n",
    "\n",
    "## まとめ\n",
    "- 目的関数は教師データと平均二乗誤差を2で割ったものと等価になります\n",
    "- numpyを使うことで簡単に計算することができます"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892e7632-40aa-44c7-8c92-5a090b8fc1ec",
   "metadata": {},
   "source": [
    "# 線形回帰の学習と推定\n",
    "\n",
    "## ゴール\n",
    "- **【Sprint 機械学習スクラッチ 線形回帰】の【問題６】を解くうえで必要な知識や技術について理解する**\n",
    "\n",
    "### Sprintの目的\n",
    "- 線形回帰の学習法について理解する\n",
    "- 学習法の実装法ついて理解する\n",
    "\n",
    "## どのように学ぶか\n",
    "\n",
    "【Sprint 機械学習スクラッチ 線形回帰】の【問題6】と照らし合わせながら、進めていきましょう。\n",
    "\n",
    "## 学習と推定\n",
    "\n",
    "ここまでの問題では、下記の関数を作成してきました。\n",
    "\n",
    "`_linear_hypothesis`：仮定関数の出力計算\n",
    "\n",
    "`_gradient_descent`：$\\theta$の更新\n",
    "\n",
    "この問題では、この2つの関数を利用し(変更可)、冒頭で示した`ScratchLinearRegression`を実装していくことを目的としています。ですので、まだ実装を行っていない`__init__()`と`fit()`を実装してみましょう。\n",
    "\n",
    "最急降下法の流れに則ると、下記の流れで実装していくとよいでしょう。\n",
    "1. 学習率や学習回数の初期化＆$\\theta$の初期化を`__init__()`で行う（`no_bias`や`verbose`については、アドバンス課題のため考慮しなくても問題ないです）\n",
    "2. 推定値算出を`_linear_hypothesis`で行う\n",
    "3. `_gradient_descent`で$\\theta$を更新する\n",
    "ここで毎回損失値を保存しておくことで学習曲線を描けるので、忘れず損失値を計算し、リストで保存しておきましょう。\n",
    "4. 2,3を学習回数分繰り返す\n",
    "\n",
    "\n",
    "\n",
    "## まとめ\n",
    "- これまで作成した関数を利用して学習や値の保持を行います\n",
    "- 1~4を行うことで、最適な$theta$に更新することができます"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002a553d-7d22-4f59-a21e-222ae5f09882",
   "metadata": {},
   "source": [
    "# 線形回帰の学習曲線のプロット\n",
    "\n",
    "## ゴール\n",
    "- **【Sprint 機械学習スクラッチ 線形回帰】の【問題7】を解くうえで必要な知識や技術について理解する**\n",
    "\n",
    "### Sprintの目的\n",
    "- 実装が正しいか確認する\n",
    "- 学習過程の可視化方法を理解する\n",
    "\n",
    "## どのように学ぶか\n",
    "\n",
    "【Sprint 機械学習スクラッチ 線形回帰】の【問題7】と照らし合わせながら、進めていきましょう。\n",
    "\n",
    "## 【問題7】学習曲線のプロット\n",
    "\n",
    "機械学習においては、学習が順調に進んでいるか確認する必要があります。学習の進捗を確認するためには、損失関数の値を可視化するのが早いです。\n",
    "\n",
    "【問題6】の過程でインスタンス変数として損失値を保持していると思います。していない場合はするようにしましょう。\n",
    "これを利用して損失値が減少していることを確認します。\n",
    "\n",
    "`train`と`test`それぞれのデータで損失値を保持し、同じグラフに出力して比較してみましょう。\n",
    "\n",
    "ヒント：グラフの描画については、前回までの講座のmatplotlibを参照してください\n",
    "\n",
    "## まとめ\n",
    "- 学習がどの様に進んでいるか確認するためにグラフ描画する必要があります\n",
    "- 実装がうまくいっているか確かめる意味でも可視化は重要になります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23264efb-403b-4542-8c86-bffca605450b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
