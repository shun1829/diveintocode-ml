{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8e8d9d-7547-4766-a4ee-6779474e6ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% md\n",
    "\n",
    "# Sprint21 自然言語処理\n",
    "\n",
    "#%% md\n",
    "\n",
    "# データの準備\n",
    "\n",
    "下記のURLから、圧縮ファイルをダウンロードしてください。\n",
    "\n",
    "http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "\n",
    "ダウンロードした圧縮ファイルを解凍し、このsprint21.ipynbと同じ階層においてください。\n",
    "\n",
    "#%% md\n",
    "\n",
    "# ライブラリのimport\n",
    "\n",
    "#%%\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import itertools\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from gensim.models import word2vec\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "import re\n",
    "\n",
    "#%% md\n",
    "\n",
    "# データの読み込み\n",
    "\n",
    "#%%\n",
    "\n",
    "train_review = load_files('./aclImdb/train/', encoding='utf-8')\n",
    "x_train, y_train = train_review.data, train_review.target\n",
    "test_review = load_files('./aclImdb/test/', encoding='utf-8')\n",
    "x_test, y_test = test_review.data, test_review.target\n",
    "\n",
    "#%%\n",
    "\n",
    "# テスト出力\n",
    "print(\"x : {}\".format(x_train[0]))\n",
    "print(np.array(x_train).shape,np.array(x_test).shape,np.array(y_train).shape,np.array(y_test).shape)\n",
    "\n",
    "#%% md\n",
    "\n",
    "# 問題1　BoWのスクラッチ実装\n",
    "\n",
    "まずは、sklearnでBoWを計算してみます。\n",
    "\n",
    "#%%\n",
    "\n",
    "# 仮のデータ\n",
    "mini_dataset = ['This movie is very good.','This film is a good','Very bad. Very, very bad.']\n",
    "\n",
    "# インスタンス化\n",
    "vectorizer = CountVectorizer(token_pattern=r'(?u)\\b\\w+\\b')\n",
    "# 実行\n",
    "bow = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
    "# DataFrame化\n",
    "df = pd.DataFrame(bow, columns=vectorizer.get_feature_names())\n",
    "# 出力\n",
    "display(df)\n",
    "\n",
    "#%% md\n",
    "\n",
    "次に、スクラッチで作ってみます。\n",
    "\n",
    "#%%\n",
    "\n",
    "def bow(data):\n",
    "    \"\"\"BoW算出\n",
    "    Parameters\n",
    "    -----------\n",
    "    data : 文章リスト\n",
    "    \"\"\"\n",
    "    ## 単語リスト作成\n",
    "    # 小文字に統一\n",
    "    # !除去\n",
    "    # 文字列を半角スペース基準で分割し、リスト化\n",
    "    row_data = [i.lower().replace('!', '').split(' ') for i in data]\n",
    "    # 1次元のリストに(単語リスト)\n",
    "    feature_names = set(list(itertools.chain.from_iterable(row_data)))\n",
    "    \n",
    "    ## bow計算\n",
    "    bow = []\n",
    "    # 1つづつ文章でループ\n",
    "    for index,row in enumerate(data):\n",
    "        bow.append([])\n",
    "        # 単語リストでループ\n",
    "        for feature_name in feature_names:\n",
    "            # 何個含まれているか\n",
    "            num = row_data[index].count(feature_name)\n",
    "            # 追加\n",
    "            bow[index].append(num)\n",
    "    return feature_names,bow\n",
    "\n",
    "# 仮データの定義\n",
    "mini_dataset = ['This movie is SOOOO funny!!!','What a movie! I never','best movie ever!!!!! this movie']\n",
    "# bow関数実行\n",
    "feature_names,bow = bow(mini_dataset)\n",
    "# DF化\n",
    "df = pd.DataFrame(bow, columns=feature_names)\n",
    "# 出力\n",
    "display(df)\n",
    "\n",
    "#%% md\n",
    "\n",
    "# 問題2　TF-IDFの計算\n",
    "\n",
    "#%%\n",
    "\n",
    "# nltkライブラリのstopwordsを利用\n",
    "stop_words = nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "print(\"stop word : {}\".format(stop_words))\n",
    "\n",
    "#%%\n",
    "\n",
    "# tfidfの算出\n",
    "vectorizer = TfidfVectorizer(stop_words= stop_words, max_features=5000)\n",
    "X_train = vectorizer.fit_transform(x_train)\n",
    "X_test = vectorizer.fit_transform(x_test)\n",
    "\n",
    "#%%\n",
    "\n",
    "# テスト出力\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "#%% md\n",
    "\n",
    "# 問題3　TF-IDFを用いた学習\n",
    "\n",
    "#%%\n",
    "\n",
    "# lightGBMを用いた学習\n",
    "lgb = lgb.LGBMClassifier().fit(X_train,y_train)\n",
    "# 推定\n",
    "y_pred = lgb.predict(X_test)\n",
    "\n",
    "#%%\n",
    "\n",
    "# 結果出力\n",
    "print(\"{}\".format(lgb.score(X_test, y_test)))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "#%% md\n",
    "\n",
    "# 問題4　TF-IDFのスクラッチ実装\n",
    "\n",
    "まずは、sklearnでtfidfを計算してみます。\n",
    "\n",
    "#%%\n",
    "\n",
    "# 仮データ\n",
    "mini_dataset = ['This movie is SOOOO funny!!!','What a movie! I never','best movie ever!!!!! this movie']\n",
    "\n",
    "#%%\n",
    "\n",
    "# インスタンス化\n",
    "tfidf_model = TfidfVectorizer()\n",
    "# 計算\n",
    "tfidf = tfidf_model.fit_transform(mini_dataset)\n",
    "# DF化\n",
    "tfidf = pd.DataFrame(tfidf.toarray(), columns=tfidf_model.get_feature_names())\n",
    "# 出力\n",
    "tfidf\n",
    "\n",
    "#%% md\n",
    "\n",
    "次に、スクラッチで作ってみます。\n",
    "\n",
    "#%%\n",
    "\n",
    "# インスタンス化\n",
    "cv_model = CountVectorizer()\n",
    "# 計算\n",
    "cv= cv_model.fit_transform(mini_dataset)\n",
    "# 扱いやすいように配列化\n",
    "cv_array = cv.toarray()\n",
    "\n",
    "# TF値計算\n",
    "N = cv_array.shape[0]\n",
    "tf = np.array([cv_array[i, :] / np.sum(cv_array, axis=1)[i] for i in range(N)])\n",
    "\n",
    "# IDF値計算\n",
    "df = np.count_nonzero(cv_array, axis=0)\n",
    "idf = np.log((1 + N) / (1 + df)) + 1\n",
    "\n",
    "# normalize\n",
    "tfidf = normalize(tf*idf)\n",
    "tfidf = pd.DataFrame(tfidf, columns=cv_model.get_feature_names())\n",
    "tfidf\n",
    "\n",
    "#%% md\n",
    "\n",
    "# 問題5　コーパスの前処理\n",
    "\n",
    "この問題以降は簡単のため、1文のみを扱う。\n",
    "\n",
    "#%%\n",
    "\n",
    "# 簡単のため、URL含んでそうな1文抜き出す\n",
    "with_url = 0\n",
    "for i, s in enumerate(x_train):\n",
    "    if 'www' in s:\n",
    "        with_url = i\n",
    "        print('-----before processing')\n",
    "        print(s)\n",
    "        break\n",
    "no_preprocessing = x_train[with_url]\n",
    "\n",
    "# urlは除外\n",
    "after_preprocessing1 = re.sub(r'https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-…]+', \"\", no_preprocessing) \n",
    "# タグ除去\n",
    "after_preprocessing2 = re.sub(r'<[^>]+>', \" \", after_preprocessing1) \n",
    "# 数字と英字以外除去\n",
    "after_preprocessing3 = re.sub(r\"[^0-9a-zA-Z ]\", \"\", after_preprocessing2) \n",
    "# 小文字に統一\n",
    "after_preprocessing = after_preprocessing3.lower() \n",
    "\n",
    "print('-----after processing')\n",
    "print(after_preprocessing)\n",
    "\n",
    "#%% md\n",
    "\n",
    "# 問題6　Word2Vecの学習\n",
    "\n",
    "#%%\n",
    "\n",
    "# 単語リスト\n",
    "word_list = [after_preprocessing.split(' ')]\n",
    "\n",
    "# vector_size: 圧縮次元数\n",
    "# min_count: 出現頻度の低いものをカットする\n",
    "# window: 前後の単語を拾う際の窓の広さを決める\n",
    "# epochs: 機械学習の繰り返し回数(デフォルト:5)十分学習できていないときにこの値を調整する\n",
    "model = word2vec.Word2Vec(word_list,min_count=1) \n",
    "\n",
    "#%%\n",
    "\n",
    "# 確認\n",
    "model.wv['hand']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
