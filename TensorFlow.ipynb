{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e804988e-81f5-4570-83a2-55118bbc7db8",
   "metadata": {},
   "source": [
    "【問題1】スクラッチを振り返る<br>\n",
    "\n",
    "【回答】<br>\n",
    "・重み、バイアスの初期化、更新<br>\n",
    "・エポックのループ<br>\n",
    "・活性化関数を通して出力する。<br>\n",
    "・forward,back propagationを繰り返す。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bea9923-4291-4e84-ad5f-41579135e9fb",
   "metadata": {},
   "source": [
    "【問題2】スクラッチとTensorFlowの対応を考える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be657168-0a5e-4d5b-ba2d-b02cfa80c33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "%matplotlib inline\n",
    "# tensorflow1系\n",
    "# import tensorflow as tf\n",
    "# tensorflow2系\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e7ca6ac-1d7a-4ff6-8136-212db8298679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 読み込み\n",
    "iris_dataset = load_iris()\n",
    "\n",
    "# 整形\n",
    "iris_dataframe = pd.DataFrame(data=iris_dataset.data, columns=iris_dataset.feature_names)\n",
    "iris_datalabel = pd.DataFrame(data=iris_dataset.target,columns=['Species'])\n",
    "df = pd.concat([iris_dataframe,iris_datalabel],axis=1)\n",
    "\n",
    "#%% md\n",
    "\n",
    "# ミニバッチクラス\n",
    "\n",
    "#%%\n",
    "\n",
    "class GetMiniBatch:\n",
    "\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baa40b27-93ae-4e0a-b29e-768a2a3e9722",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shunnadamoto/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/ipykernel_launcher.py:19: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "/Users/shunnadamoto/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/ipykernel_launcher.py:23: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train_loss : 1.1074, val_loss : 0.8296, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 1, train_loss : 1.9878, val_loss : 2.6489, train_acc : 0.531, val_acc : 0.375\n",
      "Epoch 2, train_loss : 1.3016, val_loss : 1.7220, train_acc : 0.531, val_acc : 0.375\n",
      "Epoch 3, train_loss : 2.0142, val_loss : 1.4353, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 4, train_loss : 0.6286, val_loss : 0.5327, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 5, train_loss : 0.5285, val_loss : 0.5057, train_acc : 0.953, val_acc : 0.938\n",
      "Epoch 6, train_loss : 0.5867, val_loss : 0.4912, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 7, train_loss : 0.9390, val_loss : 0.6863, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 8, train_loss : 0.9537, val_loss : 0.6915, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 9, train_loss : 0.7140, val_loss : 0.5334, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 10, train_loss : 0.6323, val_loss : 0.4785, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 11, train_loss : 0.6522, val_loss : 0.4846, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 12, train_loss : 0.6907, val_loss : 0.5034, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 13, train_loss : 0.6758, val_loss : 0.4895, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 14, train_loss : 0.6219, val_loss : 0.4511, train_acc : 0.469, val_acc : 0.625\n",
      "Epoch 15, train_loss : 0.5730, val_loss : 0.4162, train_acc : 0.500, val_acc : 0.625\n",
      "Epoch 16, train_loss : 0.5428, val_loss : 0.3932, train_acc : 0.562, val_acc : 0.625\n",
      "Epoch 17, train_loss : 0.5234, val_loss : 0.3771, train_acc : 0.562, val_acc : 0.625\n",
      "Epoch 18, train_loss : 0.5038, val_loss : 0.3610, train_acc : 0.594, val_acc : 0.750\n",
      "Epoch 19, train_loss : 0.4792, val_loss : 0.3420, train_acc : 0.656, val_acc : 0.812\n",
      "Epoch 20, train_loss : 0.4513, val_loss : 0.3213, train_acc : 0.672, val_acc : 0.875\n",
      "Epoch 21, train_loss : 0.4226, val_loss : 0.3003, train_acc : 0.719, val_acc : 0.875\n",
      "Epoch 22, train_loss : 0.3952, val_loss : 0.2805, train_acc : 0.750, val_acc : 0.875\n",
      "Epoch 23, train_loss : 0.3694, val_loss : 0.2619, train_acc : 0.828, val_acc : 0.875\n",
      "Epoch 24, train_loss : 0.3457, val_loss : 0.2449, train_acc : 0.859, val_acc : 0.938\n",
      "Epoch 25, train_loss : 0.3228, val_loss : 0.2287, train_acc : 0.891, val_acc : 0.938\n",
      "Epoch 26, train_loss : 0.3012, val_loss : 0.2136, train_acc : 0.922, val_acc : 0.938\n",
      "Epoch 27, train_loss : 0.2805, val_loss : 0.1994, train_acc : 0.938, val_acc : 0.938\n",
      "Epoch 28, train_loss : 0.2621, val_loss : 0.1867, train_acc : 0.938, val_acc : 1.000\n",
      "Epoch 29, train_loss : 0.2453, val_loss : 0.1753, train_acc : 0.953, val_acc : 1.000\n",
      "Epoch 30, train_loss : 0.2301, val_loss : 0.1650, train_acc : 0.969, val_acc : 1.000\n",
      "Epoch 31, train_loss : 0.2166, val_loss : 0.1558, train_acc : 0.969, val_acc : 1.000\n",
      "Epoch 32, train_loss : 0.2047, val_loss : 0.1476, train_acc : 0.969, val_acc : 1.000\n",
      "Epoch 33, train_loss : 0.1938, val_loss : 0.1402, train_acc : 0.969, val_acc : 1.000\n",
      "Epoch 34, train_loss : 0.1838, val_loss : 0.1333, train_acc : 0.969, val_acc : 1.000\n",
      "Epoch 35, train_loss : 0.1748, val_loss : 0.1271, train_acc : 0.984, val_acc : 1.000\n",
      "Epoch 36, train_loss : 0.1666, val_loss : 0.1214, train_acc : 0.984, val_acc : 1.000\n",
      "Epoch 37, train_loss : 0.1590, val_loss : 0.1161, train_acc : 0.984, val_acc : 1.000\n",
      "Epoch 38, train_loss : 0.1519, val_loss : 0.1112, train_acc : 0.984, val_acc : 1.000\n",
      "Epoch 39, train_loss : 0.1454, val_loss : 0.1066, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 40, train_loss : 0.1394, val_loss : 0.1024, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 41, train_loss : 0.1339, val_loss : 0.0984, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 42, train_loss : 0.1287, val_loss : 0.0947, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 43, train_loss : 0.1237, val_loss : 0.0911, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 44, train_loss : 0.1191, val_loss : 0.0878, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 45, train_loss : 0.1148, val_loss : 0.0847, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 46, train_loss : 0.1108, val_loss : 0.0817, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 47, train_loss : 0.1069, val_loss : 0.0789, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 48, train_loss : 0.1033, val_loss : 0.0762, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 49, train_loss : 0.0999, val_loss : 0.0737, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 50, train_loss : 0.0966, val_loss : 0.0712, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 51, train_loss : 0.0935, val_loss : 0.0689, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 52, train_loss : 0.0906, val_loss : 0.0667, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 53, train_loss : 0.0878, val_loss : 0.0646, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 54, train_loss : 0.0851, val_loss : 0.0626, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 55, train_loss : 0.0826, val_loss : 0.0607, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 56, train_loss : 0.0801, val_loss : 0.0588, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 57, train_loss : 0.0778, val_loss : 0.0570, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 58, train_loss : 0.0756, val_loss : 0.0553, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 59, train_loss : 0.0734, val_loss : 0.0537, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 60, train_loss : 0.0714, val_loss : 0.0521, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 61, train_loss : 0.0694, val_loss : 0.0506, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 62, train_loss : 0.0675, val_loss : 0.0492, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 63, train_loss : 0.0657, val_loss : 0.0478, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 64, train_loss : 0.0640, val_loss : 0.0464, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 65, train_loss : 0.0623, val_loss : 0.0451, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 66, train_loss : 0.0607, val_loss : 0.0439, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 67, train_loss : 0.0592, val_loss : 0.0427, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 68, train_loss : 0.0577, val_loss : 0.0415, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 69, train_loss : 0.0562, val_loss : 0.0404, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 70, train_loss : 0.0549, val_loss : 0.0393, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 71, train_loss : 0.0535, val_loss : 0.0383, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 72, train_loss : 0.0522, val_loss : 0.0373, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 73, train_loss : 0.0510, val_loss : 0.0363, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 74, train_loss : 0.0498, val_loss : 0.0354, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 75, train_loss : 0.0486, val_loss : 0.0345, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 76, train_loss : 0.0475, val_loss : 0.0336, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 77, train_loss : 0.0464, val_loss : 0.0327, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 78, train_loss : 0.0453, val_loss : 0.0319, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 79, train_loss : 0.0443, val_loss : 0.0311, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 80, train_loss : 0.0433, val_loss : 0.0304, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 81, train_loss : 0.0424, val_loss : 0.0296, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 82, train_loss : 0.0414, val_loss : 0.0289, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 83, train_loss : 0.0406, val_loss : 0.0282, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 84, train_loss : 0.0397, val_loss : 0.0275, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 85, train_loss : 0.0388, val_loss : 0.0269, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 86, train_loss : 0.0380, val_loss : 0.0262, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 87, train_loss : 0.0372, val_loss : 0.0256, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 88, train_loss : 0.0365, val_loss : 0.0250, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 89, train_loss : 0.0357, val_loss : 0.0245, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 90, train_loss : 0.0350, val_loss : 0.0239, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 91, train_loss : 0.0343, val_loss : 0.0234, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 92, train_loss : 0.0336, val_loss : 0.0228, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 93, train_loss : 0.0329, val_loss : 0.0223, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 94, train_loss : 0.0323, val_loss : 0.0218, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 95, train_loss : 0.0316, val_loss : 0.0213, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 96, train_loss : 0.0310, val_loss : 0.0209, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 97, train_loss : 0.0304, val_loss : 0.0204, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 98, train_loss : 0.0298, val_loss : 0.0200, train_acc : 1.000, val_acc : 1.000\n",
      "Epoch 99, train_loss : 0.0293, val_loss : 0.0195, train_acc : 1.000, val_acc : 1.000\n",
      "test_acc : 1.000\n"
     ]
    }
   ],
   "source": [
    "#%% md\n",
    "\n",
    "# 問題2　スクラッチとTensorFlowの対応を考える\n",
    "\n",
    "#%% md\n",
    "\n",
    "## データ準備\n",
    "\n",
    "#%%\n",
    "\n",
    "# 2値分類のため絞り込み\n",
    "df2 = df[(df[\"Species\"] == 0)|(df[\"Species\"] == 1)]\n",
    "\n",
    "# 説明変数と目的変数に分割\n",
    "y = df2[\"Species\"]\n",
    "X = df2.loc[:, [\"sepal length (cm)\", \"sepal width (cm)\", \"petal length (cm)\", \"petal width (cm)\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "\n",
    "# 訓練データ/テストデータ/評価データに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# 正規化\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "X_val /= 255\n",
    "\n",
    "#%% md\n",
    "\n",
    "## tensorflowで学習\n",
    "\n",
    "#%%\n",
    "\n",
    "# 各種変数定義\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 100\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1\n",
    "\n",
    "# 空配列定義\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# ミニバッチイテレータ生成\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \"\"\"tensorflowを利用したニューラルネットワーク\n",
    "    Parameters\n",
    "    ---------------\n",
    "    x : 入力配列\n",
    "    \"\"\"\n",
    "    # 重み定義\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    # バイアス定義\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    # 計算グラフ構築（順伝播処理）\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
    "    return layer_output\n",
    "\n",
    "# 計算グラフ受け取る\n",
    "logits = example_net(X)\n",
    "# 損失定義\n",
    "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "# 最適化手法の定義\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "# 最適化手法で、定義した損失を最小化するルールを作成\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# ACC計算\n",
    "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# 変数を扱うためのおまじない\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# tensorflowのセッション開始\n",
    "with tf.Session() as sess:\n",
    "    # 初期化\n",
    "    sess.run(init)\n",
    "    # 学習回数分ループ\n",
    "    for epoch in range(num_epochs):\n",
    "        # ミニバッチイテレータでループ\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # 「最適化手法で、定義した損失を最小化するルール」を実行\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "        # 記録\n",
    "        train_loss, train_acc = sess.run([loss_op, accuracy], feed_dict={X: X_train, Y: y_train})\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        # 仮定出力\n",
    "        print(\"Epoch {}, train_loss : {:.4f}, val_loss : {:.4f}, train_acc : {:.3f}, val_acc : {:.3f}\".format(epoch, train_loss, val_loss, train_acc, val_acc))\n",
    "    \n",
    "    # 学習が終了したらテストデータで実行\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    #prediction = sess.run(logits, feed_dict={X: X_test, Y: y_test})\n",
    "    #print(prediction)\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9179d40-57bc-44a3-8b26-f77a431550fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shunnadamoto/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/ipykernel_launcher.py:16: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  app.launch_new_instance()\n",
      "/Users/shunnadamoto/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/ipykernel_launcher.py:23: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train_loss : 4.5107, val_loss : 4.6722, train_acc : 0.312, val_acc : 0.292\n",
      "Epoch 1, train_loss : 2.0330, val_loss : 2.0369, train_acc : 0.365, val_acc : 0.375\n",
      "Epoch 2, train_loss : 3.2035, val_loss : 3.1690, train_acc : 0.323, val_acc : 0.333\n",
      "Epoch 3, train_loss : 2.5130, val_loss : 2.4887, train_acc : 0.323, val_acc : 0.333\n",
      "Epoch 4, train_loss : 1.5210, val_loss : 1.5295, train_acc : 0.323, val_acc : 0.333\n",
      "Epoch 5, train_loss : 1.3187, val_loss : 1.3660, train_acc : 0.312, val_acc : 0.292\n",
      "Epoch 6, train_loss : 1.3918, val_loss : 1.4506, train_acc : 0.312, val_acc : 0.292\n",
      "Epoch 7, train_loss : 1.3348, val_loss : 1.3924, train_acc : 0.312, val_acc : 0.292\n",
      "Epoch 8, train_loss : 1.2263, val_loss : 1.2798, train_acc : 0.312, val_acc : 0.292\n",
      "Epoch 9, train_loss : 1.2437, val_loss : 1.2979, train_acc : 0.312, val_acc : 0.292\n",
      "Epoch 10, train_loss : 1.1768, val_loss : 1.2283, train_acc : 0.312, val_acc : 0.292\n",
      "Epoch 11, train_loss : 0.9736, val_loss : 1.0143, train_acc : 0.604, val_acc : 0.583\n",
      "Epoch 12, train_loss : 0.8057, val_loss : 0.8398, train_acc : 0.656, val_acc : 0.583\n",
      "Epoch 13, train_loss : 0.6553, val_loss : 0.6722, train_acc : 0.896, val_acc : 0.875\n",
      "Epoch 14, train_loss : 0.6584, val_loss : 0.6815, train_acc : 0.844, val_acc : 0.750\n",
      "Epoch 15, train_loss : 0.6749, val_loss : 0.7032, train_acc : 0.823, val_acc : 0.750\n",
      "Epoch 16, train_loss : 0.6298, val_loss : 0.6534, train_acc : 0.854, val_acc : 0.750\n",
      "Epoch 17, train_loss : 0.6164, val_loss : 0.6404, train_acc : 0.854, val_acc : 0.750\n",
      "Epoch 18, train_loss : 0.6175, val_loss : 0.6440, train_acc : 0.844, val_acc : 0.750\n",
      "Epoch 19, train_loss : 0.6048, val_loss : 0.6309, train_acc : 0.802, val_acc : 0.750\n",
      "Epoch 20, train_loss : 0.5898, val_loss : 0.6160, train_acc : 0.802, val_acc : 0.750\n",
      "Epoch 21, train_loss : 0.5816, val_loss : 0.6083, train_acc : 0.812, val_acc : 0.750\n",
      "Epoch 22, train_loss : 0.5750, val_loss : 0.6018, train_acc : 0.802, val_acc : 0.792\n",
      "Epoch 23, train_loss : 0.5649, val_loss : 0.5915, train_acc : 0.802, val_acc : 0.792\n",
      "Epoch 24, train_loss : 0.5548, val_loss : 0.5828, train_acc : 0.802, val_acc : 0.792\n",
      "Epoch 25, train_loss : 0.5457, val_loss : 0.5743, train_acc : 0.812, val_acc : 0.792\n",
      "Epoch 26, train_loss : 0.5404, val_loss : 0.5685, train_acc : 0.812, val_acc : 0.792\n",
      "Epoch 27, train_loss : 0.5313, val_loss : 0.5605, train_acc : 0.812, val_acc : 0.792\n",
      "Epoch 28, train_loss : 0.5217, val_loss : 0.5514, train_acc : 0.812, val_acc : 0.792\n",
      "Epoch 29, train_loss : 0.5137, val_loss : 0.5434, train_acc : 0.812, val_acc : 0.792\n",
      "Epoch 30, train_loss : 0.5068, val_loss : 0.5378, train_acc : 0.823, val_acc : 0.792\n",
      "Epoch 31, train_loss : 0.5019, val_loss : 0.5334, train_acc : 0.823, val_acc : 0.792\n",
      "Epoch 32, train_loss : 0.4962, val_loss : 0.5288, train_acc : 0.823, val_acc : 0.792\n",
      "Epoch 33, train_loss : 0.4862, val_loss : 0.5186, train_acc : 0.823, val_acc : 0.833\n",
      "Epoch 34, train_loss : 0.4743, val_loss : 0.5073, train_acc : 0.844, val_acc : 0.833\n",
      "Epoch 35, train_loss : 0.4665, val_loss : 0.4997, train_acc : 0.844, val_acc : 0.833\n",
      "Epoch 36, train_loss : 0.4586, val_loss : 0.4923, train_acc : 0.854, val_acc : 0.833\n",
      "Epoch 37, train_loss : 0.4559, val_loss : 0.4909, train_acc : 0.854, val_acc : 0.833\n",
      "Epoch 38, train_loss : 0.4482, val_loss : 0.4836, train_acc : 0.854, val_acc : 0.833\n",
      "Epoch 39, train_loss : 0.4402, val_loss : 0.4750, train_acc : 0.875, val_acc : 0.875\n",
      "Epoch 40, train_loss : 0.4310, val_loss : 0.4675, train_acc : 0.896, val_acc : 0.875\n",
      "Epoch 41, train_loss : 0.4230, val_loss : 0.4596, train_acc : 0.896, val_acc : 0.875\n",
      "Epoch 42, train_loss : 0.4142, val_loss : 0.4510, train_acc : 0.917, val_acc : 0.875\n",
      "Epoch 43, train_loss : 0.4068, val_loss : 0.4456, train_acc : 0.938, val_acc : 0.875\n",
      "Epoch 44, train_loss : 0.4034, val_loss : 0.4415, train_acc : 0.938, val_acc : 0.875\n",
      "Epoch 45, train_loss : 0.3952, val_loss : 0.4359, train_acc : 0.938, val_acc : 0.917\n",
      "Epoch 46, train_loss : 0.3907, val_loss : 0.4310, train_acc : 0.938, val_acc : 0.917\n",
      "Epoch 47, train_loss : 0.3810, val_loss : 0.4232, train_acc : 0.938, val_acc : 0.917\n",
      "Epoch 48, train_loss : 0.3778, val_loss : 0.4196, train_acc : 0.938, val_acc : 0.917\n",
      "Epoch 49, train_loss : 0.3686, val_loss : 0.4118, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 50, train_loss : 0.3655, val_loss : 0.4092, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 51, train_loss : 0.3601, val_loss : 0.4050, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 52, train_loss : 0.3575, val_loss : 0.4038, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 53, train_loss : 0.3541, val_loss : 0.4008, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 54, train_loss : 0.3475, val_loss : 0.3966, train_acc : 0.958, val_acc : 0.875\n",
      "Epoch 55, train_loss : 0.3509, val_loss : 0.3999, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 56, train_loss : 0.3528, val_loss : 0.4022, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 57, train_loss : 0.3491, val_loss : 0.4005, train_acc : 0.969, val_acc : 0.917\n",
      "Epoch 58, train_loss : 0.3469, val_loss : 0.3982, train_acc : 0.969, val_acc : 0.917\n",
      "Epoch 59, train_loss : 0.3375, val_loss : 0.3914, train_acc : 0.958, val_acc : 0.958\n",
      "Epoch 60, train_loss : 0.3434, val_loss : 0.3964, train_acc : 0.948, val_acc : 0.917\n",
      "Epoch 61, train_loss : 0.3398, val_loss : 0.3921, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 62, train_loss : 0.3246, val_loss : 0.3801, train_acc : 0.969, val_acc : 0.917\n",
      "Epoch 63, train_loss : 0.3303, val_loss : 0.3855, train_acc : 0.948, val_acc : 0.958\n",
      "Epoch 64, train_loss : 0.3231, val_loss : 0.3776, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 65, train_loss : 0.3025, val_loss : 0.3596, train_acc : 0.979, val_acc : 0.917\n",
      "Epoch 66, train_loss : 0.3055, val_loss : 0.3636, train_acc : 0.979, val_acc : 0.917\n",
      "Epoch 67, train_loss : 0.3015, val_loss : 0.3601, train_acc : 0.979, val_acc : 0.917\n",
      "Epoch 68, train_loss : 0.2868, val_loss : 0.3487, train_acc : 0.979, val_acc : 0.917\n",
      "Epoch 69, train_loss : 0.3066, val_loss : 0.3707, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 70, train_loss : 0.3111, val_loss : 0.3742, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 71, train_loss : 0.2831, val_loss : 0.3491, train_acc : 0.969, val_acc : 0.917\n",
      "Epoch 72, train_loss : 0.3046, val_loss : 0.3743, train_acc : 0.938, val_acc : 0.917\n",
      "Epoch 73, train_loss : 0.3175, val_loss : 0.3871, train_acc : 0.927, val_acc : 0.917\n",
      "Epoch 74, train_loss : 0.2928, val_loss : 0.3664, train_acc : 0.958, val_acc : 0.875\n",
      "Epoch 75, train_loss : 0.3391, val_loss : 0.4191, train_acc : 0.906, val_acc : 0.875\n",
      "Epoch 76, train_loss : 0.4017, val_loss : 0.4799, train_acc : 0.823, val_acc : 0.750\n",
      "Epoch 77, train_loss : 0.3512, val_loss : 0.4279, train_acc : 0.875, val_acc : 0.875\n",
      "Epoch 78, train_loss : 0.2993, val_loss : 0.3845, train_acc : 0.927, val_acc : 0.833\n",
      "Epoch 79, train_loss : 0.2771, val_loss : 0.3598, train_acc : 0.958, val_acc : 0.875\n",
      "Epoch 80, train_loss : 0.2032, val_loss : 0.2857, train_acc : 0.969, val_acc : 0.875\n",
      "Epoch 81, train_loss : 0.1830, val_loss : 0.2695, train_acc : 0.969, val_acc : 0.875\n",
      "Epoch 82, train_loss : 0.2122, val_loss : 0.2467, train_acc : 0.938, val_acc : 0.917\n",
      "Epoch 83, train_loss : 0.2087, val_loss : 0.2493, train_acc : 0.917, val_acc : 0.917\n",
      "Epoch 84, train_loss : 0.1631, val_loss : 0.2353, train_acc : 0.958, val_acc : 0.917\n",
      "Epoch 85, train_loss : 0.1611, val_loss : 0.2247, train_acc : 0.979, val_acc : 0.917\n",
      "Epoch 86, train_loss : 0.1609, val_loss : 0.2195, train_acc : 0.958, val_acc : 0.958\n",
      "Epoch 87, train_loss : 0.1572, val_loss : 0.2215, train_acc : 0.979, val_acc : 0.917\n",
      "Epoch 88, train_loss : 0.1531, val_loss : 0.2207, train_acc : 0.979, val_acc : 0.917\n",
      "Epoch 89, train_loss : 0.1514, val_loss : 0.2171, train_acc : 0.979, val_acc : 0.917\n",
      "Epoch 90, train_loss : 0.1493, val_loss : 0.2164, train_acc : 0.979, val_acc : 0.917\n",
      "Epoch 91, train_loss : 0.1471, val_loss : 0.2160, train_acc : 0.979, val_acc : 0.917\n",
      "Epoch 92, train_loss : 0.1453, val_loss : 0.2140, train_acc : 0.979, val_acc : 0.917\n",
      "Epoch 93, train_loss : 0.1434, val_loss : 0.2132, train_acc : 0.979, val_acc : 0.917\n",
      "Epoch 94, train_loss : 0.1417, val_loss : 0.2124, train_acc : 0.979, val_acc : 0.917\n",
      "Epoch 95, train_loss : 0.1400, val_loss : 0.2111, train_acc : 0.979, val_acc : 0.917\n",
      "Epoch 96, train_loss : 0.1384, val_loss : 0.2103, train_acc : 0.979, val_acc : 0.917\n",
      "Epoch 97, train_loss : 0.1369, val_loss : 0.2094, train_acc : 0.979, val_acc : 0.917\n",
      "Epoch 98, train_loss : 0.1354, val_loss : 0.2086, train_acc : 0.969, val_acc : 0.917\n",
      "Epoch 99, train_loss : 0.1340, val_loss : 0.2078, train_acc : 0.969, val_acc : 0.917\n",
      "test_acc : 1.000\n"
     ]
    }
   ],
   "source": [
    "#%% md\n",
    "\n",
    "# 問題3　3種類すべての目的変数を使用したIrisのモデルを作成\n",
    "\n",
    "#%% md\n",
    "\n",
    "## データ準備\n",
    "\n",
    "#%%\n",
    "\n",
    "# 説明変数と目的変数に分割\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"sepal length (cm)\", \"sepal width (cm)\", \"petal length (cm)\", \"petal width (cm)\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "\n",
    "# 訓練データ/テストデータ/評価データに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# onehotベクトル化\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train)\n",
    "y_val_one_hot = enc.transform(y_val)\n",
    "y_test_one_hot = enc.transform(y_test)\n",
    "\n",
    "# 正規化\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "X_val /= 255\n",
    "\n",
    "#%% md\n",
    "\n",
    "## tensorflowで学習\n",
    "\n",
    "#%%\n",
    "\n",
    "# 各種変数定義\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 100\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 3 # 2値分類からの変更箇所\n",
    "\n",
    "# 空配列定義\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# ミニバッチイテレータ生成\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train_one_hot, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \"\"\"tensorflowを利用したニューラルネットワーク\n",
    "    Parameters\n",
    "    ---------------\n",
    "    x : 入力配列\n",
    "    \"\"\"\n",
    "    # 重み定義\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    # バイアス定義\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    # 計算グラフ構築（順伝播処理）\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
    "    return layer_output\n",
    "\n",
    "# 計算グラフ受け取る\n",
    "logits = example_net(X)\n",
    "# 損失定義\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits)) # 2値分類からの変更箇所\n",
    "# 最適化手法の定義\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "# 最適化手法で、定義した損失を最小化するルールを作成\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# ACC計算\n",
    "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1)) # 2値分類からの変更箇所\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# 変数を扱うためのおまじない\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# tensorflowのセッション開始\n",
    "with tf.Session() as sess:\n",
    "    # 初期化\n",
    "    sess.run(init)\n",
    "    # 学習回数分ループ\n",
    "    for epoch in range(num_epochs):\n",
    "        # ミニバッチイテレータでループ\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # 「最適化手法で、定義した損失を最小化するルール」を実行\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "        # 記録\n",
    "        train_loss, train_acc = sess.run([loss_op, accuracy], feed_dict={X: X_train, Y: y_train_one_hot})\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val_one_hot})\n",
    "        # 仮定出力\n",
    "        print(\"Epoch {}, train_loss : {:.4f}, val_loss : {:.4f}, train_acc : {:.3f}, val_acc : {:.3f}\".format(epoch, train_loss, val_loss, train_acc, val_acc))\n",
    "    \n",
    "    # 学習が終了したらテストデータで実行\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
    "    #prediction = sess.run(logits, feed_dict={X: X_test, Y: y_test})\n",
    "    #print(prediction)\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b10a3823-f104-401f-ada9-f23f39aa66cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shunnadamoto/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/ipykernel_launcher.py:18: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "/Users/shunnadamoto/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/ipykernel_launcher.py:23: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 17480896.0000, val_loss : 15796340.0000\n",
      "Epoch 1, loss : 3409438.0000, val_loss : 2381235.5000\n",
      "Epoch 2, loss : 1949718.7500, val_loss : 1279496.8750\n",
      "Epoch 3, loss : 1211029.1250, val_loss : 769370.8750\n",
      "Epoch 4, loss : 792286.1875, val_loss : 463125.5938\n",
      "Epoch 5, loss : 519826.9062, val_loss : 260110.6562\n",
      "Epoch 6, loss : 348251.9375, val_loss : 149723.4375\n",
      "Epoch 7, loss : 253131.0312, val_loss : 100077.8203\n",
      "Epoch 8, loss : 190137.8125, val_loss : 76798.9375\n",
      "Epoch 9, loss : 143641.3750, val_loss : 60984.2070\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_mse : 368400.219\n"
     ]
    }
   ],
   "source": [
    "#%% md\n",
    "\n",
    "# 問題4　House Pricesのモデルを作成\n",
    "\n",
    "#%% md\n",
    "\n",
    "## データ準備\n",
    "\n",
    "#%%\n",
    "\n",
    "dataset_path =\"train.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "y = df[\"SalePrice\"]\n",
    "X = df.loc[:, [\"GrLivArea\", \"YearBuilt\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "y = np.log(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "#%% md\n",
    "\n",
    "## tensorflowで学習\n",
    "\n",
    "#%%\n",
    "\n",
    "# 各種変数定義\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1\n",
    "\n",
    "# 空配列定義\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# ミニバッチイテレータ生成\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \"\"\"tensorflowを利用したニューラルネットワーク\n",
    "    Parameters\n",
    "    ---------------\n",
    "    x : 入力配列\n",
    "    \"\"\"\n",
    "    # 重み定義\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    # バイアス定義\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    # 計算グラフ構築（順伝播処理）\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
    "    return layer_output\n",
    "\n",
    "# 計算グラフ受け取る\n",
    "logits = example_net(X)\n",
    "# 損失定義\n",
    "loss_op =  tf.losses.mean_squared_error(labels=Y, predictions=logits)\n",
    "# 最適化手法の定義\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "# 最適化手法で、定義した損失を最小化するルールを作成\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# 変数を扱うためのおまじない\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# tensorflowのセッション開始\n",
    "with tf.Session() as sess:\n",
    "    # 初期化\n",
    "    sess.run(init)\n",
    "    # 損失記録用リスト\n",
    "    loss_list = []\n",
    "    val_loss_list = []\n",
    "    # 学習回数分ループ\n",
    "    for epoch in range(num_epochs):\n",
    "        # ミニバッチイテレータでループ\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # 「最適化手法で、定義した損失を最小化するルール」を実行\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "        # 損失計算と格納\n",
    "        loss = sess.run(loss_op, feed_dict={X: X_train, Y: y_train})\n",
    "        val_loss = sess.run(loss_op, feed_dict={X: X_val, Y: y_val})\n",
    "        loss_list.append(loss)\n",
    "        val_loss_list.append(val_loss)    \n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}\".format(epoch, loss, val_loss))\n",
    "    \n",
    "    # 学習過程可視化\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.plot(loss_list, label='loss')\n",
    "    plt.plot(val_loss_list, label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # テストデータに適用\n",
    "    test_loss = sess.run(loss_op, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_mse : {:.3f}\".format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "176247f3-180b-41a8-8065-6b61a547c2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shunnadamoto/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/ipykernel_launcher.py:19: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "/Users/shunnadamoto/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/ipykernel_launcher.py:20: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "/Users/shunnadamoto/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/ipykernel_launcher.py:25: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "/Users/shunnadamoto/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "/Users/shunnadamoto/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/ipykernel_launcher.py:23: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train_loss : 36.9310, val_loss : 36.7000, train_acc : 0.603, val_acc : 0.604\n",
      "Epoch 1, train_loss : 25.5549, val_loss : 25.5309, train_acc : 0.687, val_acc : 0.684\n",
      "Epoch 2, train_loss : 25.1535, val_loss : 25.4318, train_acc : 0.692, val_acc : 0.691\n",
      "Epoch 3, train_loss : 23.0572, val_loss : 22.7927, train_acc : 0.718, val_acc : 0.716\n",
      "Epoch 4, train_loss : 20.0919, val_loss : 20.4625, train_acc : 0.755, val_acc : 0.752\n",
      "Epoch 5, train_loss : 18.2065, val_loss : 18.6572, train_acc : 0.775, val_acc : 0.768\n",
      "Epoch 6, train_loss : 19.2109, val_loss : 19.8238, train_acc : 0.784, val_acc : 0.780\n",
      "Epoch 7, train_loss : 24.6006, val_loss : 24.7160, train_acc : 0.758, val_acc : 0.757\n",
      "Epoch 8, train_loss : 19.0896, val_loss : 19.2712, train_acc : 0.786, val_acc : 0.784\n",
      "Epoch 9, train_loss : 19.0852, val_loss : 19.7189, train_acc : 0.801, val_acc : 0.801\n",
      "Epoch 10, train_loss : 20.5119, val_loss : 21.6866, train_acc : 0.790, val_acc : 0.783\n",
      "Epoch 11, train_loss : 17.8528, val_loss : 18.4235, train_acc : 0.805, val_acc : 0.800\n",
      "Epoch 12, train_loss : 21.2858, val_loss : 22.4286, train_acc : 0.786, val_acc : 0.784\n",
      "Epoch 13, train_loss : 22.2108, val_loss : 22.5547, train_acc : 0.786, val_acc : 0.791\n",
      "Epoch 14, train_loss : 22.7846, val_loss : 23.8093, train_acc : 0.808, val_acc : 0.801\n",
      "Epoch 15, train_loss : 18.1032, val_loss : 18.4189, train_acc : 0.832, val_acc : 0.833\n",
      "Epoch 16, train_loss : 17.1681, val_loss : 17.4815, train_acc : 0.840, val_acc : 0.839\n",
      "Epoch 17, train_loss : 21.1422, val_loss : 21.1585, train_acc : 0.824, val_acc : 0.818\n",
      "Epoch 18, train_loss : 20.4571, val_loss : 20.8612, train_acc : 0.835, val_acc : 0.833\n",
      "Epoch 19, train_loss : 21.7107, val_loss : 21.7641, train_acc : 0.834, val_acc : 0.829\n",
      "test_acc : 0.832\n"
     ]
    }
   ],
   "source": [
    "#%% md\n",
    "\n",
    "# 問題5　MNISTのモデルを作成\n",
    "\n",
    "#%% md\n",
    "\n",
    "# データ準備\n",
    "\n",
    "#%%\n",
    "\n",
    "# 読み込み\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#　平滑化\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "# 正規化\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# 変形\n",
    "y_train = y_train.astype(np.int)[:, np.newaxis]\n",
    "y_test = y_test.astype(np.int)[:, np.newaxis]\n",
    "\n",
    "# one-hotベクトル化\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:])\n",
    "y_test_one_hot = enc.fit_transform(y_test[:])\n",
    "\n",
    "# 分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
    "\n",
    "#%% md\n",
    "\n",
    "## tensorflowで学習\n",
    "\n",
    "#%%\n",
    "\n",
    "# 各種変数定義\n",
    "learning_rate = 0.003\n",
    "batch_size = 1\n",
    "num_epochs = 20\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 10 # 2値分類からの変更箇所\n",
    "\n",
    "# 空配列定義\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# ミニバッチイテレータ生成\n",
    "get_mini_batch_train = GetMiniBatch(X_train[:1000], y_train[:1000], batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \"\"\"tensorflowを利用したニューラルネットワーク\n",
    "    Parameters\n",
    "    ---------------\n",
    "    x : 入力配列\n",
    "    \"\"\"\n",
    "    # 重み定義\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    # バイアス定義\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    # 計算グラフ構築（順伝播処理）\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
    "    return layer_output\n",
    "\n",
    "# 計算グラフ受け取る\n",
    "logits = example_net(X)\n",
    "# 損失定義\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits)) # 2値分類からの変更箇所\n",
    "# 最適化手法の定義\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "# 最適化手法で、定義した損失を最小化するルールを作成\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# ACC計算\n",
    "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1)) # 2値分類からの変更箇所\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# 変数を扱うためのおまじない\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# tensorflowのセッション開始\n",
    "with tf.Session() as sess:\n",
    "    # 初期化\n",
    "    sess.run(init)\n",
    "    # 学習回数分ループ\n",
    "    for epoch in range(num_epochs):\n",
    "        # ミニバッチイテレータでループ\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # 「最適化手法で、定義した損失を最小化するルール」を実行\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "        # 記録\n",
    "        train_loss, train_acc = sess.run([loss_op, accuracy], feed_dict={X: X_train, Y: y_train})\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        # 仮定出力\n",
    "        print(\"Epoch {}, train_loss : {:.4f}, val_loss : {:.4f}, train_acc : {:.3f}, val_acc : {:.3f}\".format(epoch, train_loss, val_loss, train_acc, val_acc))\n",
    "    \n",
    "    # 学習が終了したらテストデータで実行\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
    "    #prediction = sess.run(logits, feed_dict={X: X_test, Y: y_test})\n",
    "    #print(prediction)\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))\n",
    "\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151f56e8-f765-4d41-8076-6092564ebeb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
